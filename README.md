# Wav2Vec2 inference on ONNX runtime

This repository contains a simple example of how to run inference on a Wav2Vec2 model using ONNX runtime. The model is first converted to ONNX format using the `torch.onnx.export` function and then loaded into ONNX runtime for inference.

## Usage

Run `npm run dev` to start the vite development server. Run the model, check timings in the console.

